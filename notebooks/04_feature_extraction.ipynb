{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c57d76aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed frames: D:\\VidSense\\data\\processed\n",
      "Feature output: D:\\VidSense\\data\\features\n"
     ]
    }
   ],
   "source": [
    "# ---\n",
    "# FEATURE EXTRACTION – CNN (VGG16)\n",
    "# Senior Engineer Notes:\n",
    "# - Extract deep CNN features from each frame\n",
    "# - Saveas numpy arrays for LSTM model\n",
    "# -----------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Detect project root (consistent with previous notebooks)\n",
    "repo_root = Path.cwd().parent if \"notebooks\" in str(Path.cwd()) else Path.cwd()\n",
    "\n",
    "processed_dir = repo_root / \"data\" / \"processed\"\n",
    "features_dir = repo_root / \"data\" / \"features\"\n",
    "\n",
    "features_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Processed frames:\", processed_dir)\n",
    "print(\"Feature output:\", features_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80fcf544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 model loaded. Output feature dim: (None, 512)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# Load pretrained VGG16model\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# base VGG16\n",
    "base_model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "print(\"VGG16 model loaded. Output feature dim:\", model.output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f16ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------\n",
    "# Extract CNN features from a single frame\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def extract_feature_from_frame(frame):\n",
    "    frame_resized = cv2.resize(frame, (224, 224))\n",
    "    frame_array = np.expand_dims(frame_resized.astype(\"float32\"), axis=0)\n",
    "    frame_processed = preprocess_input(frame_array)\n",
    "    feature = model.predict(frame_processed, verbose=0)\n",
    "    return feature.flatten()  # shape: (512,--)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63094feb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected labels: ['falling', 'running', 'sitting', 'walking']\n",
      "\n",
      "Processing label: falling | 3 videos\n",
      "✔ Saved features for: falling_demo_1 → (12, 512)\n",
      "✔ Saved features for: falling_demo_2 → (12, 512)\n",
      "✔ Saved features for: falling_demo_3 → (12, 512)\n",
      "\n",
      "Processing label: running | 3 videos\n",
      "✔ Saved features for: running_demo_1 → (12, 512)\n",
      "✔ Saved features for: running_demo_2 → (12, 512)\n",
      "✔ Saved features for: running_demo_3 → (12, 512)\n",
      "\n",
      "Processing label: sitting | 3 videos\n",
      "✔ Saved features for: sitting_demo_1 → (12, 512)\n",
      "✔ Saved features for: sitting_demo_2 → (12, 512)\n",
      "✔ Saved features for: sitting_demo_3 → (12, 512)\n",
      "\n",
      "Processing label: walking | 3 videos\n",
      "✔ Saved features for: walking_demo_1 → (12, 512)\n",
      "✔ Saved features for: walking_demo_2 → (12, 512)\n",
      "✔ Saved features for: walking_demo_3 → (12, 512)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# Loop through processed frames folder and extract features\n",
    "# -----------------------------------------------------\n",
    "\n",
    "labels = [d.name for d in processed_dir.iterdir() if d.is_dir()]\n",
    "print(\"Detected labels:\", labels)\n",
    "\n",
    "for label in labels:\n",
    "    label_features_dir = features_dir / label\n",
    "    label_features_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    label_frame_dir = processed_dir / label\n",
    "    videos = [v for v in label_frame_dir.iterdir() if v.is_dir()]\n",
    "\n",
    "    print(f\"\\nProcessing label: {label} | {len(videos)} videos\")\n",
    "\n",
    "    for vid_folder in videos:\n",
    "        frames = sorted(list(vid_folder.glob(\"*.jpg\")))\n",
    "\n",
    "        if len(frames) == 0:\n",
    "            continue\n",
    "\n",
    "        features_list = []\n",
    "\n",
    "        for frame_path in frames:\n",
    "            frame = cv2.imread(str(frame_path))\n",
    "            feat = extract_feature_from_frame(frame)\n",
    "            features_list.append(feat)\n",
    "\n",
    "        video_features = np.array(features_list)  # shape: (num_frames, 512)\n",
    "\n",
    "        out_path = label_features_dir / f\"{vid_folder.name}.npy\"\n",
    "        np.save(out_path, video_features)\n",
    "\n",
    "        print(f\"✔ Saved features for: {vid_folder.name} → {video_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca53831d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SUMMARY =======\n",
      "\n",
      "Label: falling\n",
      "  falling_demo_1.npy: (12, 512)\n",
      "  falling_demo_2.npy: (12, 512)\n",
      "  falling_demo_3.npy: (12, 512)\n",
      "\n",
      "Label: running\n",
      "  running_demo_1.npy: (12, 512)\n",
      "  running_demo_2.npy: (12, 512)\n",
      "  running_demo_3.npy: (12, 512)\n",
      "\n",
      "Label: sitting\n",
      "  sitting_demo_1.npy: (12, 512)\n",
      "  sitting_demo_2.npy: (12, 512)\n",
      "  sitting_demo_3.npy: (12, 512)\n",
      "\n",
      "Label: walking\n",
      "  walking_demo_1.npy: (12, 512)\n",
      "  walking_demo_2.npy: (12, 512)\n",
      "  walking_demo_3.npy: (12, 512)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== SUMMARY =======\")\n",
    "\n",
    "for label in labels:\n",
    "    label_dir = features_dir / label\n",
    "    if not label_dir.exists():\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nLabel: {label}\")\n",
    "    for npy_file in label_dir.glob(\"*.npy\"):\n",
    "        data = np.load(npy_file)\n",
    "        print(f\"  {npy_file.name}: {data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcaa4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (VidSense)",
   "language": "python",
   "name": "vidsense_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
